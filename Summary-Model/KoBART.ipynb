{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_metric\n",
    "from transformers import PreTrainedTokenizerFast, BartForConditionalGeneration, Trainer, TrainingArguments\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 불러오기\n",
    "ds = load_dataset(\"daekeun-ml/naver-news-summarization-ko\")\n",
    "\n",
    "# 데이터셋 확인\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 준비\n",
    "train_data = ds['train']\n",
    "val_data = ds['validation']\n",
    "\n",
    "# 토크나이저 로드\n",
    "model_name = 'gogamza/kobart-summarization'  # KoBART 요약 모델\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(model_name)\n",
    "\n",
    "# 데이터 전처리\n",
    "def preprocess_function(examples):\n",
    "    inputs = tokenizer(examples['document'], max_length=1024, truncation=True, padding=\"max_length\")\n",
    "    targets = tokenizer(examples['summary'], max_length=128, truncation=True, padding=\"max_length\")\n",
    "    \n",
    "    inputs[\"labels\"] = targets[\"input_ids\"]  # labels는 target summary의 input_ids\n",
    "    return inputs\n",
    "\n",
    "# 데이터셋 전처리\n",
    "train_dataset = train_data.map(preprocess_function, batched=True)\n",
    "val_dataset = val_data.map(preprocess_function, batched=True)\n",
    "\n",
    "# 모델 로드\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Trainer 설정\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # 출력 디렉토리\n",
    "    evaluation_strategy=\"epoch\",     # 평가 전략\n",
    "    learning_rate=2e-5,              # 학습률\n",
    "    per_device_train_batch_size=8,   # 학습 배치 사이즈\n",
    "    per_device_eval_batch_size=8,    # 평가 배치 사이즈\n",
    "    num_train_epochs=3,              # 학습 에폭 수\n",
    "    weight_decay=0.01,               # 가중치 감소\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                         # 모델\n",
    "    args=training_args,                  # 학습 인자\n",
    "    train_dataset=train_dataset,         # 학습 데이터셋\n",
    "    eval_dataset=val_dataset,            # 평가 데이터셋\n",
    ")\n",
    "\n",
    "# 모델 학습\n",
    "trainer.train()\n",
    "\n",
    "# 모델 저장\n",
    "model.save_pretrained(\"./fine_tuned_kobart\")\n",
    "tokenizer.save_pretrained(\"./fine_tuned_kobart\")\n",
    "\n",
    "# ROUGE metric 불러오기\n",
    "rouge_metric = load_metric(\"rouge\", trust_remote_code=True)\n",
    "\n",
    "# 평가 함수 정의\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    labels = np.where(labels != tokenizer.pad_token_id, labels, -100)  # pad token을 -100으로 설정\n",
    "    pred_ids = torch.argmax(torch.tensor(logits), dim=-1)  # 예측값 생성\n",
    "    predictions = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)  # 예측값 디코딩\n",
    "    references = tokenizer.batch_decode(labels, skip_special_tokens=True)  # 실제 값 디코딩\n",
    "    \n",
    "    # ROUGE 평가 지표 계산\n",
    "    result = rouge_metric.compute(predictions=predictions, references=references)\n",
    "    return result\n",
    "\n",
    "# 평가 진행\n",
    "trainer.compute_metrics = compute_metrics\n",
    "\n",
    "# 모델 평가\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "# 평가 결과 출력\n",
    "print(eval_results)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
